# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11JEIDUIzcsgaiJv4mqKlmJP-wMPsl8wd
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.decomposition import PCA
import numpy as np

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score

train_df = pd.read_csv("./train.csv")
test_df  = pd.read_csv("./test.csv")

"""# Task 1: EDA (Exploratory Data Analysis)."""

print('Shape:', train_df.shape)
display(train_df.head())
display(train_df.describe(include='all'))

# Missing values summary
missing = train_df.isnull().sum().sort_values(ascending=False)
missing = missing[missing > 0]
print('Columns with missing values (count):')
print(missing)

# If none, show a message
if missing.empty:
    print('No missing values found')

# Target distribution
plt.figure(figsize=(6,4))
sns.countplot(x='satisfaction', data=train_df)
plt.title('Target distribution (satisfaction)')
plt.show()
print(train_df['satisfaction'].value_counts())
print('Normalized:')
print(train_df['satisfaction'].value_counts(normalize=True))

# Categorical columns distribution (if any remain as object)
cat_cols = train_df.select_dtypes(include='object').columns.tolist()
print('Categorical columns:', cat_cols)
for c in cat_cols[:6]:
    plt.figure(figsize=(6,3))
    sns.countplot(y=c, data=train_df, order=train_df[c].value_counts().index)
    plt.title(f'Counts of {c}')
    plt.tight_layout()
    plt.show()

# Numerical distributions and boxplots to check outliers
num_cols = ['Age','Flight Distance','Departure Delay in Minutes','Arrival Delay in Minutes']
plt.figure(figsize=(12,8))
for i, col in enumerate(num_cols, 1):
    plt.subplot(2,2,i)
    sns.histplot(train_df[col].dropna(), kde=True)
    plt.title(col)
plt.tight_layout()
plt.show()

plt.figure(figsize=(12,8))
for i, col in enumerate(num_cols, 1):
    plt.subplot(2,2,i)
    sns.boxplot(x='satisfaction', y=col, data=train_df)
    plt.title(f'Boxplot of {col} by satisfaction')
plt.tight_layout()
plt.show()

# Correlation heatmap (numeric features) — only numeric columns, handle NaNs and add mask
num_df = train_df.select_dtypes(include=[np.number])
if num_df.shape[1] == 0:
    print('No numeric columns available for correlation')
else:
    corr = num_df.corr()
    if corr.isnull().values.any():
        corr = corr.fillna(0)
    mask = np.triu(np.ones_like(corr, dtype=bool))
    plt.figure(figsize=(12,10))
    sns.heatmap(corr, annot=True, fmt=".2f", cmap='coolwarm', mask=mask, vmax=1.0, vmin=-1.0, linewidths=.5)
    plt.title('Correlation matrix (numeric features)')
    plt.show()

sample = train_df[num_cols + ['satisfaction']].dropna().sample(min(500, len(train_df)), random_state=42)
sns.pairplot(sample, hue='satisfaction', diag_kind='kde')
plt.show()

"""# Task 2: Preprocessing"""

# ===============================
# 2️⃣ Drop unwanted columns
# ===============================
cols_to_drop = ['Unnamed: 0', 'id']
for df_ in [train_df, test_df]:
    df_.drop(columns=[c for c in cols_to_drop if c in df_.columns], inplace=True)

# ===============================
# 3️⃣ Mapping categorical values to numbers (with astype to remove warning)
# ===============================
# Target mapping (فقط روی train)
target_map = {'satisfied': 1, 'neutral or dissatisfied': 0}
if 'satisfaction' in train_df.columns:
    train_df['satisfaction'] = train_df['satisfaction'].replace(target_map).astype(int)

# Binary categorical
binary_maps = {
    'Gender': {'Male': 1, 'Female': 0},
    'Customer Type': {'Loyal Customer': 1, 'disloyal Customer': 0},
    'Type of Travel': {'Business travel': 1, 'Personal Travel': 0}
}
for col, m in binary_maps.items():
    if col in train_df.columns:
        train_df[col] = train_df[col].replace(m).astype(int)
        test_df[col] = test_df[col].replace(m).astype(int)

# Ordinal categorical
class_map = {'Business': 2, 'Eco Plus': 1, 'Eco': 0}
if 'Class' in train_df.columns:
    train_df['Class'] = train_df['Class'].replace(class_map).astype(int)
    test_df['Class'] = test_df['Class'].replace(class_map).astype(int)

# ===============================
# 4️⃣ Fill missing numeric values (median from train)
# ===============================
num_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()
for c in num_cols:
    median_val = train_df[c].median()
    train_df[c] = train_df[c].fillna(median_val)
    if c in test_df.columns:
        test_df[c] = test_df[c].fillna(median_val)

# ===============================
# 5️⃣ Separate X and y
# ===============================
X_train = train_df.drop('satisfaction', axis=1)
y_train = train_df['satisfaction']

if 'satisfaction' in test_df.columns:
    X_test = test_df.drop('satisfaction', axis=1)
    y_test = test_df['satisfaction']
else:
    X_test = test_df.copy()
    y_test = None  # فقط برای پیش‌بینی

# ===============================
# 6️⃣ StandardScaler on numeric features
# ===============================
num_cols_X = X_train.select_dtypes(include=[np.number]).columns.tolist()
scaler = StandardScaler()

# fit on train, transform both train and test
X_train[num_cols_X] = scaler.fit_transform(X_train[num_cols_X])
X_test[num_cols_X] = scaler.transform(X_test[num_cols_X])

# ===============================
# 7️⃣ Quick summaries
# ===============================
print("Train shape:", X_train.shape, y_train.shape)
print("Test shape:", X_test.shape, None if y_test is None else y_test.shape)
print("Target distribution (train):")
print(y_train.value_counts(normalize=True))

"""# Task 3: Extract meaningful features and use them to classify dataset"""

# ===============================
# 1️⃣ Prepare features and target
# ===============================
X_feat = X_train.copy()  # از train استفاده می‌کنیم
y_feat = y_train.astype(int)

print('Train features shape:', X_feat.shape, 'Train target shape:', y_feat.shape)

# ===============================
# 2️⃣ Base classifier
# ===============================
base_clf = LogisticRegression(max_iter=1000, solver='lbfgs')
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
n_select = min(10, X_feat.shape[1])

# ===============================
# 3️⃣ Forward Selection
# ===============================
sfs_fwd = SequentialFeatureSelector(
    base_clf,
    n_features_to_select=n_select,
    direction='forward',
    scoring='f1',
    cv=skf,
    n_jobs=-1
)
sfs_fwd.fit(X_feat, y_feat)
fwd_features = X_feat.columns[sfs_fwd.get_support()].tolist()
print('Forward selected features (n=%d):' % len(fwd_features))
print(fwd_features)
fwd_score = cross_val_score(base_clf, X_feat[fwd_features], y_feat, cv=skf, scoring='f1', n_jobs=-1)
print('Forward selection F1: %.4f (± %.4f)' % (fwd_score.mean(), fwd_score.std()))

# ===============================
# 4️⃣ Backward Elimination
# ===============================
sfs_bwd = SequentialFeatureSelector(
    base_clf,
    n_features_to_select=n_select,
    direction='backward',
    scoring='f1',
    cv=skf,
    n_jobs=-1
)
sfs_bwd.fit(X_feat, y_feat)
bwd_features = X_feat.columns[sfs_bwd.get_support()].tolist()
print('Backward selected features (n=%d):' % len(bwd_features))
print(bwd_features)
bwd_score = cross_val_score(base_clf, X_feat[bwd_features], y_feat, cv=skf, scoring='f1', n_jobs=-1)
print('Backward selection F1: %.4f (± %.4f)' % (bwd_score.mean(), bwd_score.std()))

# ===============================
# 5️⃣ Common features
# ===============================
common = set(fwd_features).intersection(bwd_features)
print('Common features between forward and backward (%d):' % len(common), sorted(common))

# ===============================
# 6️⃣ PCA on train
# ===============================
pca = PCA(n_components=0.95, random_state=42)
X_pca = pca.fit_transform(X_feat)
print('Original features:', X_feat.shape[1])
print('PCA components retained:', X_pca.shape[1])

# Evaluate with same base classifier using CV
pca_score = cross_val_score(base_clf, X_pca, y_feat, cv=skf, scoring='f1', n_jobs=-1)
print('PCA-based F1: %.4f (± %.4f)' % (pca_score.mean(), pca_score.std()))

# Show explained variance ratio top components
print('Explained variance ratio (first 10 components):', np.round(pca.explained_variance_ratio_[:10], 4))

# ===============================
# 7️⃣ Apply feature selection / PCA to test
# ===============================
# Forward selected
X_test_fwd = X_test[fwd_features]

# Backward selected
X_test_bwd = X_test[bwd_features]

# Common selected
X_test_common = X_test[list(common)]

# PCA-transformed test
X_test_pca = pca.transform(X_test)

"""# Task 4: Classify with K-Nearest Neighbors (KNN)."""

# ===============================
# 1) Mapping target in train & test to numeric
# ===============================
target_map = {'satisfied': 1, 'neutral or dissatisfied': 0}

# Train
y_train = train_df['satisfaction'].replace(target_map).astype('int64')

# Test
if 'satisfaction' in test_df.columns:
    y_test = test_df['satisfaction'].replace(target_map).astype('int64')
else:
    y_test = None  # فقط برای پیش‌بینی

# ===============================
# 2) Feature sets
# ===============================
common_features = ['Checkin service','Class','Customer Type','Inflight wifi service',
                   'Leg room service','On-board service','Online boarding','Type of Travel']

backward_features = ['Customer Type','Type of Travel','Class','Inflight wifi service',
                     'Departure/Arrival time convenient','Online boarding','On-board service',
                     'Leg room service','Checkin service','Cleanliness']

# ===============================
# 3) KNN evaluation function
# ===============================
def evaluate_knn_train_test(X_train, y_train, X_test, y_test, feature_list, k=5):
    print('--- Evaluating KNN (k=%d) on %d features ---' % (k, len(feature_list)))

    # ستون‌های انتخابی
    X_train_sub = X_train[feature_list]
    X_test_sub = X_test[feature_list]

    # مدل KNN
    clf = KNeighborsClassifier(n_neighbors=k, n_jobs=-1)
    clf.fit(X_train_sub, y_train)

    # پیش‌بینی روی تست
    y_pred = clf.predict(X_test_sub)

    # گزارش معیارها
    print('Test accuracy:', accuracy_score(y_test, y_pred))
    print('Test F1:', f1_score(y_test, y_pred))
    print('Classification report:')
    print(classification_report(y_test, y_pred, digits=4))
    print('Confusion matrix:\n', confusion_matrix(y_test, y_pred))

    return clf

# ===============================
# 4) اجرای ارزیابی روی feature set ها
# ===============================
print('Evaluating common features:')
clf_common = evaluate_knn_train_test(X_train, y_train, X_test, y_test, common_features, k=5)

print('Evaluating backward-selected features:')
clf_bwd = evaluate_knn_train_test(X_train, y_train, X_test, y_test, backward_features, k=5)

"""# Task 5: Find optimal K."""

# ===============================
# Target mapping (train & test)
# ===============================
target_map = {'satisfied': 1, 'neutral or dissatisfied': 0}
y_train_num = train_df['satisfaction'].replace(target_map).astype(int)
y_test_num = test_df['satisfaction'].replace(target_map).astype(int)

# ===============================
# Feature sets
# ===============================
common_features = ['Checkin service','Class','Customer Type','Inflight wifi service',
                   'Leg room service','On-board service','Online boarding','Type of Travel']

backward_features = ['Customer Type','Type of Travel','Class','Inflight wifi service',
                     'Departure/Arrival time convenient','Online boarding','On-board service',
                     'Leg room service','Checkin service','Cleanliness']

# ===============================
# Grid search & evaluation function
# ===============================
ks = list(range(1,52,2))  # odd k from 1..51
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

def grid_search_and_eval_train_test(X_train, y_train, X_test, y_test, features, name):
    print(f'=== Grid search for {name} (features={len(features)}) ===')
    X_sub_train = X_train[features]
    X_sub_test = X_test[features]

    param_grid = {'n_neighbors': ks}
    grid = GridSearchCV(KNeighborsClassifier(), param_grid, scoring='f1', cv=cv, n_jobs=-1)
    grid.fit(X_sub_train, y_train)

    best_k = grid.best_params_['n_neighbors']
    best_score = grid.best_score_
    print(f'Best k (CV) for {name}:', best_k, 'CV F1 =', round(best_score,4))

    # extract mean test scores for plotting
    mean_scores = []
    for k in ks:
        idx = list(grid.cv_results_['params']).index({'n_neighbors': k})
        mean_scores.append(grid.cv_results_['mean_test_score'][idx])
    plt.figure(figsize=(8,4))
    plt.plot(ks, mean_scores, marker='o')
    plt.xlabel('k')
    plt.ylabel('CV F1')
    plt.title(f'Grid search CV F1 vs k ({name})')
    plt.grid(True)
    plt.show()

    # evaluate on hold-out test set
    clf = KNeighborsClassifier(n_neighbors=best_k)
    clf.fit(X_sub_train, y_train)
    y_pred = clf.predict(X_sub_test)

    print('Test accuracy:', accuracy_score(y_test, y_pred))
    print('Test F1:', f1_score(y_test, y_pred))
    print('Classification report:')
    print(classification_report(y_test, y_pred, digits=4))
    print('Confusion matrix:')
    print(confusion_matrix(y_test, y_pred))
    return best_k, best_score

# ===============================
# Run grid search and evaluation on both feature sets
# ===============================
best_k_common, best_score_common = grid_search_and_eval_train_test(
    train_df, y_train_num, test_df, y_test_num, common_features, 'common_features'
)

best_k_bwd, best_score_bwd = grid_search_and_eval_train_test(
    train_df, y_train_num, test_df, y_test_num, backward_features, 'backward_features'
)

print('Finished Task 5')

"""# Task 6: Classify with Decision Tree Classifier with ID3, C4.5"""

# Ensure target is numeric
target_map = {'satisfied': 1, 'neutral or dissatisfied': 0}
y_train_num = train_df['satisfaction'].replace(target_map).astype(int)
y_test_num = test_df['satisfaction'].replace(target_map).astype(int)

# Feature sets
sets = {
    'common': common_features,
    'backward': backward_features
}

results = {}

for name, feats in sets.items():
    print('='*60)
    print(f'Feature set: {name} (n_features={len(feats)})')

    X_train_sub = train_df[feats].copy()
    X_test_sub = test_df[feats].copy()

    # --------------------------------------------------
    # ID3 — Entropy, no pruning
    # --------------------------------------------------
    id3 = DecisionTreeClassifier(
        criterion='entropy',
        random_state=42
    )
    id3.fit(X_train_sub, y_train_num)
    y_pred_id3 = id3.predict(X_test_sub)

    print('\nID3 Results:')
    print(' Accuracy:', accuracy_score(y_test_num, y_pred_id3))
    print(' F1-score:', f1_score(y_test_num, y_pred_id3))
    print(' Confusion Matrix:\n', confusion_matrix(y_test_num, y_pred_id3))
    print(classification_report(y_test_num, y_pred_id3, digits=4))

    results[(name, 'ID3')] = {
        'model': id3,
        'accuracy': accuracy_score(y_test_num, y_pred_id3),
        'f1': f1_score(y_test_num, y_pred_id3)
    }

    # --------------------------------------------------
    # C4.5-like — Entropy + Pruning (sklearn approximation)
    # --------------------------------------------------
    c45 = DecisionTreeClassifier(
        criterion='entropy',
        max_depth=8,            # pruning
        min_samples_leaf=20,    # pruning
        random_state=42
    )
    c45.fit(X_train_sub, y_train_num)
    y_pred_c45 = c45.predict(X_test_sub)

    print('\nC4.5-like Results:')
    print(' Accuracy:', accuracy_score(y_test_num, y_pred_c45))
    print(' F1-score:', f1_score(y_test_num, y_pred_c45))
    print(' Confusion Matrix:\n', confusion_matrix(y_test_num, y_pred_c45))
    print(classification_report(y_test_num, y_pred_c45, digits=4))

    results[(name, 'C4.5-like')] = {
        'model': c45,
        'accuracy': accuracy_score(y_test_num, y_pred_c45),
        'f1': f1_score(y_test_num, y_pred_c45)
    }

print('\nTask 6 complete — ID3 and C4.5-like models evaluated successfully.')

"""# Task 7: Classify with Bagging / Boosting methods"""

# Task 7: Bagging and Boosting evaluations using train_df/test_df
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix

# Ensure target is numeric
target_map = {'satisfied': 1, 'neutral or dissatisfied': 0}
y_train_num = train_df['satisfaction'].replace(target_map).astype(int)
y_test_num = test_df['satisfaction'].replace(target_map).astype(int)

# Feature sets
sets = {'common': common_features, 'backward': backward_features}

# Ensemble models
models = {
    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),
    'BaggingDT': BaggingClassifier(estimator=DecisionTreeClassifier(random_state=42), n_estimators=50, random_state=42, n_jobs=-1),
    'AdaBoost': AdaBoostClassifier(n_estimators=50, random_state=42),
    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42)
}

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
results_ens = {}

for set_name, feats in sets.items():
    print('=== Feature set:', set_name, '(%d features)' % len(feats))
    X_train_sub = train_df[feats].copy()
    X_test_sub = test_df[feats].copy()
    y_train_sub = y_train_num.copy()
    y_test_sub = y_test_num.copy()

    for mname, m in models.items():
        print('-- Model:', mname)
        m.fit(X_train_sub, y_train_sub)
        y_pred = m.predict(X_test_sub)

        acc = accuracy_score(y_test_sub, y_pred)
        f1 = f1_score(y_test_sub, y_pred)
        print(' Test acc: %.4f  Test F1: %.4f' % (acc, f1))
        print(classification_report(y_test_sub, y_pred, digits=4))
        print('Confusion matrix:\n', confusion_matrix(y_test_sub, y_pred))

        # CV F1 (5-fold) on train set for robustness
        try:
            cv_scores = cross_val_score(m, X_train_sub, y_train_sub, cv=skf, scoring='f1', n_jobs=-1)
            print(' CV F1 (train): %.4f (± %.4f)' % (cv_scores.mean(), cv_scores.std()))
        except Exception as e:
            print(' CV scoring failed:', e)
        print()

        results_ens[(set_name, mname)] = {'acc': acc, 'f1': f1}

print('Task 7 complete — ensemble methods evaluated')

"""# Task 9: enhance performance of your models"""

# ============================================================
# Task 9 — Model Performance Enhancement (train_df/test_df)
# ============================================================

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    f1_score,
    accuracy_score,
    precision_recall_curve
)
import numpy as np

# ------------------------------------------------------------
# 0) Prepare features and numeric target
# ------------------------------------------------------------
target_map = {'satisfied': 1, 'neutral or dissatisfied': 0}
y_train_num = train_df['satisfaction'].replace(target_map).astype(int)
y_test_num = test_df['satisfaction'].replace(target_map).astype(int)

X_train_sub = train_df[backward_features].copy()
X_test_sub = test_df[backward_features].copy()

# ------------------------------------------------------------
# 1) Hyperparameter tuning with class balancing
# ------------------------------------------------------------
rf = RandomForestClassifier(random_state=42)

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_leaf': [1, 5, 10],
    'class_weight': [None, 'balanced']
}

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

grid = GridSearchCV(
    rf,
    param_grid,
    scoring='f1',
    cv=cv,
    n_jobs=-1
)

grid.fit(X_train_sub, y_train_num)

best_rf = grid.best_estimator_

print('Best hyperparameters:')
print(grid.best_params_)
print('Best CV F1-score:', round(grid.best_score_, 4))

# ------------------------------------------------------------
# 2) Evaluation on test set with default threshold (0.5)
# ------------------------------------------------------------
y_pred_default = best_rf.predict(X_test_sub)

print('\nEvaluation with default threshold (0.5):')
print('Accuracy:', accuracy_score(y_test_num, y_pred_default))
print('F1-score:', f1_score(y_test_num, y_pred_default))
print('Confusion matrix:\n', confusion_matrix(y_test_num, y_pred_default))
print(classification_report(y_test_num, y_pred_default, digits=4))

# ------------------------------------------------------------
# 3) Threshold tuning to maximize F1-score
# ------------------------------------------------------------
y_probs = best_rf.predict_proba(X_test_sub)[:, 1]

precisions, recalls, thresholds = precision_recall_curve(y_test_num, y_probs)
f1_scores = (2 * precisions * recalls) / (precisions + recalls + 1e-9)

best_idx = np.argmax(f1_scores[:-1])
best_threshold = thresholds[best_idx]

y_pred_tuned = (y_probs >= best_threshold).astype(int)

print('\nEvaluation with tuned threshold:')
print('Best threshold:', round(best_threshold, 4))
print('F1-score (tuned):', f1_score(y_test_num, y_pred_tuned))
print('Confusion matrix:\n', confusion_matrix(y_test_num, y_pred_tuned))
print(classification_report(y_test_num, y_pred_tuned, digits=4))

print('\nTask 9 complete — model performance enhanced successfully.')

